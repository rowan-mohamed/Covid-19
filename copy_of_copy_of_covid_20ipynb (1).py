# -*- coding: utf-8 -*-
"""Copy of Copy of Covid_20ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d4zLKIMUZe1ObRWK5A9TplowEpT_6M92
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow==2.2.0
!pip install Keras==2.2.0
!pip install tensorflow==1.14
!pip install Keras==2.0.8
!pip install Keras-Applications
!pip install keras_applications==1.0.4
!pip install tensorflow
!pip install keras==2.3.1

from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger
from keras.models import Model

from keras.models import Sequential

from keras.layers import Dense, Dropout, Flatten

from keras.layers import Conv2D, MaxPooling2D

from keras import backend as K

from keras.preprocessing.image import ImageDataGenerator

from keras.applications.vgg16 import VGG16

import keras.metrics

from keras.optimizers import Adam, RMSprop

import numpy as np





from sklearn.metrics import classification_report, confusion_matrix

from sklearn.datasets import make_classification

from sklearn.model_selection import train_test_split

from sklearn.metrics import roc_curve



from sklearn.metrics import auc


#import matplotlib.pyplot as plt


import numpy, scipy.io

img_height=224

img_width=224

def bring_data_from_directory():

# create a data generator

 datagen = ImageDataGenerator()

 batch_size=128

# load and iterate training dataset

 train_it = datagen.flow_from_directory('/content/drive/MyDrive/DATA /TRAIN',target_size=(img_height, img_width),class_mode='categorical', batch_size=batch_size)

 val_it = datagen.flow_from_directory('/content/drive/MyDrive/DATA /VALI',target_size=(img_height, img_width),class_mode='categorical', batch_size=batch_size)
 return train_it,val_it

def load_VGG16_model():
  base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))
  print ("Model loaded..!")
  print (base_model.summary())
  return base_model

def extract_features_and_store(train_generator,validation_generator,base_model):
  
  x_generator = None
  y_lable = None
  batch = 0
  for x,y in train_generator:
        if batch == (int(31268/batch_size)):#elmafroud yewsal 3nd 244 and then y save a file
            break
        print ("predict on batch:",batch)
        print((31268/batch_size))
        

        if batch==0:
          #  if np.array([]).size: print(1)
          x_generator = base_model.predict_on_batch(x)
          y_lable = y
          #  print (y)
          print('x')
        else:
          x_generator = np.append(x_generator,base_model.predict_on_batch(x),axis=0)
          y_lable = np.append(y_lable,y,axis=0)
          print('g')
        batch+=1
    
        print('s')
    # x_generator,y_lable = shuffle(x_generator,y_lable)
  np.save('/content/drive/MyDrive/train_x1.npy',x_generator)
  np.save('/content/drive/MyDrive/train_y1.npy',y_lable)
  batch = 0
  x_generator = None
  y_lable = None
  # return None
  for x,y in validation_generator:
      if batch == int(14696/batch_size):#elmafroud ywsal 3nd 114 and then y save a file 
          break
      print ("predict on batch validate:",batch)
      
      if batch==0:
         x_generator = base_model.predict_on_batch(x)
         y_lable = y
      else:
        x_generator = np.append(x_generator,base_model.predict_on_batch(x),axis=0)
        y_lable = np.append(y_lable,y,axis=0)
      batch+=1
  #  x_generator,y_lable = shuffle(x_generator,y_lable)
  np.save('Vali_x1.npy',x_generator)
  np.save('Vali_y1.npy',y_lable)
  return None

model=load_VGG16_model()

# model=load_VGG16_model()
        # extract_features_and_store(train_it ,val_it,model)

# model=load_VGG16_model()
# extract_features_and_store(train_it,val_it,model)

# model=load_VGG16_model()
# extract_features_and_store(train_it,val_it,model)

# model=load_VGG16_model()
# extract_features_and_store(train_it,val_it,model)

'''
this cell is for opening the file after it's saved inshallah
'''
# np.load('/content/train_x.npy', mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')

from sklearn.utils import shuffle

def extract_features_and_store(train_generator,validation_generator,base_model):
  train_data = np.load('/content/drive/MyDrive/train_x.npy',allow_pickle=True)
  train_labels = np.load('/content/drive/MyDrive/train_y.npy',allow_pickle=True)
  train_data,train_labels = shuffle(train_data,train_labels)
  validation_data = np.load('/content/drive/MyDrive/Vali_x.npy',allow_pickle=True)
  validation_labels = np.load('/content/drive/MyDrive/Vali_U.npy',allow_pickle=True)
  validation_data,validation_labels = shuffle(validation_data,validation_labels)

  train_data = train_data.reshape(train_data.shape[0],
                     train_data.shape[1] * train_data.shape[2],
                     train_data.shape[3])
  validation_data = validation_data.reshape(validation_data.shape[0],
                     validation_data.shape[1] * validation_data.shape[2],
                     validation_data.shape[3])
  
  return train_data,train_labels,validation_data,validation_labels

train_data = np.load('/content/drive/MyDrive/train_x.npy',allow_pickle=True)

print(train_data.shape)

print(train_data.reshape(train_data.shape[0],
                     train_data.shape[1] * train_data.shape[2],
                     train_data.shape[3]))

def categorical_focal_loss(gamma=2.0, alpha=0.25):
    """
    Implementation of Focal Loss from the paper in multiclass classification
    Formula:
        loss = -alpha*((1-p)^gamma)*log(p)
    Parameters:
        alpha -- the same as wighting factor in balanced cross entropy
        gamma -- focusing parameter for modulating factor (1-p)
    Default value:
        gamma -- 2.0 as mentioned in the paper
        alpha -- 0.25 as mentioned in the paper
    """
    def focal_loss(y_true, y_pred):
        # Define epsilon so that the backpropagation will not result in NaN
        # for 0 divisor case
        epsilon = K.epsilon()
        # Add the epsilon to prediction value
        #y_pred = y_pred + epsilon
        # Clip the prediction value
        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)
        # Calculate cross entropy
        cross_entropy = -y_true*K.log(y_pred)
        # Calculate weight that consists of  modulating factor and weighting factor
        weight = alpha * y_true * K.pow((1-y_pred), gamma)
        # Calculate focal loss
        loss = weight * cross_entropy
        # Sum the losses in mini_batch
        loss = K.sum(loss, axis=1)
        return loss
    print("focal loss done")
    
    return focal_loss

def train_model(train_data,train_labels,validation_data,validation_labels):
  ''' used fully connected layers, SGD optimizer and 
      checkpoint to store the best weights'''

  model = Sequential()
  model.add(LSTM(256,dropout=0.2,input_shape=(train_data.shape[1],
                     train_data.shape[2])))
  model.add(Dense(1024, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(3, activation='softmax'))
  sgd = SGD(lr=0.0001, decay = 1e-6, momentum=0.9, nesterov=True)
  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])
  #model.load_weights('video_1_LSTM_1_512.h5')
  callbacks = [ EarlyStopping(monitor='val_loss', patience=10, verbose=0), ModelCheckpoint('/content/drive/MyDrive/LSTM_4_1024.h5', monitor='val_loss', save_best_only=True, verbose=0) ]
  nb_epoch = 500
  model.fit(train_data,train_labels,validation_data=(validation_data,validation_labels),batch_size=batch_size,nb_epoch=nb_epoch,callbacks=callbacks,shuffle=True,verbose=1)
  return None

train_generator,validation_generator = bring_data_from_directory()
  base_model = load_VGG16_model()
  train_data,train_labels,validation_data,validation_labels =  extract_features_and_store(train_generator,validation_generator,model)

  train_model(train_data,train_labels,validation_data,validation_labels)
  
  # base_model = load_VGG16_model()
  # train_data,train_labels,validation_data,validation_labels =  extract_features_and_store(train_it,val_it,model)
  # train_model(train_data,train_labels,validation_data,validation_labels)

categorical_focal_loss(gamma=2.0, alpha=0.25)

from keras.optimizers import  SGD
from keras.layers import Dense,LSTM
batch_size=128

train_model(train_data,train_labels,validation_data,validation_labels)

